{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7b5ccfcf-2ae3-4d4a-9bba-44d642c42b54",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-10T14:53:40.001167Z",
     "iopub.status.busy": "2024-10-10T14:53:40.000527Z",
     "iopub.status.idle": "2024-10-10T14:53:43.595826Z",
     "shell.execute_reply": "2024-10-10T14:53:43.594972Z",
     "shell.execute_reply.started": "2024-10-10T14:53:40.001112Z"
    },
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import gc\n",
    "import cv2\n",
    "import time\n",
    "import json\n",
    "import paddle\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import paddle.nn as nn\n",
    "import matplotlib.pyplot as plt\n",
    "import paddle.nn.functional as F\n",
    "from paddle.io import DataLoader, TensorDataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7c336d72-8b14-44a7-a489-271adcd78857",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-08-19T13:22:14.609339Z",
     "iopub.status.busy": "2024-08-19T13:22:14.608434Z",
     "iopub.status.idle": "2024-08-19T13:22:14.789547Z",
     "shell.execute_reply": "2024-08-19T13:22:14.788268Z",
     "shell.execute_reply.started": "2024-08-19T13:22:14.609298Z"
    },
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "class Identity(nn.Layer):\n",
    "    \"\"\" Identity layer\n",
    "    The output of this layer is the input without any change.\n",
    "    This layer is used to avoid using 'if' condition in methods such as forward\n",
    "    \"\"\"\n",
    "    def forward(self, x):\n",
    "        return x\n",
    "\n",
    "\n",
    "class PatchEmbedding(nn.Layer):\n",
    "    \"\"\"Patch Embedding\n",
    "    Apply patch embedding (which is implemented using Conv2D) on input data.\n",
    "\n",
    "    Attributes:\n",
    "        image_size: image size\n",
    "        patch_size: patch size\n",
    "        num_patches: num of patches\n",
    "        patch_embddings: patch embed operation (Conv2D)\n",
    "    \"\"\"\n",
    "    def __init__(self,\n",
    "                 image_size=224,\n",
    "                 patch_size=16,\n",
    "                 in_channels=3,\n",
    "                 embed_dim=768):\n",
    "        super().__init__()\n",
    "        self.image_size = image_size\n",
    "        self.patch_size = patch_size\n",
    "        self.num_patches = (image_size // patch_size) * (image_size // patch_size)\n",
    "        self.patch_embedding = nn.Conv2D(in_channels=in_channels,\n",
    "                                         out_channels=embed_dim,\n",
    "                                         kernel_size=patch_size,\n",
    "                                         stride=patch_size)\n",
    "    def forward(self, x):\n",
    "        x = self.patch_embedding(x)\n",
    "        x = x.flatten(2)  # [B, C, H, W] -> [B, C, h*w]\n",
    "        x = x.transpose([0, 2, 1])  # [B, C, h*w] -> [B, h*w, C] = [B, N, C]\n",
    "        return x\n",
    "\n",
    "\n",
    "class Attention(nn.Layer):\n",
    "    \"\"\" Attention module\n",
    "    Attention module for ViT, here q, k, v are assumed the same.\n",
    "    The qkv mappings are stored as one single param.\n",
    "\n",
    "    Attributes:\n",
    "        num_heads: number of heads\n",
    "        attn_head_size: feature dim of single head\n",
    "        all_head_size: feature dim of all heads\n",
    "        qkv: a nn.Linear for q, k, v mapping\n",
    "        scales: 1 / sqrt(single_head_feature_dim)\n",
    "        out: projection of multi-head attention\n",
    "        attn_dropout: dropout for attention\n",
    "        proj_dropout: final dropout before output\n",
    "        softmax: softmax op for attention\n",
    "    \"\"\"\n",
    "    def __init__(self,\n",
    "                 embed_dim,\n",
    "                 num_heads,\n",
    "                 attn_head_size=None,\n",
    "                 qkv_bias=True,\n",
    "                 dropout=0.,\n",
    "                 attention_dropout=0.):\n",
    "        super().__init__()\n",
    "        self.embed_dim = embed_dim\n",
    "        self.num_heads = num_heads\n",
    "        if attn_head_size is not None:\n",
    "            self.attn_head_size = attn_head_size\n",
    "        else:\n",
    "            assert embed_dim % num_heads == 0, \"embed_dim must be divisible by num_heads\"\n",
    "            self.attn_head_size = embed_dim // num_heads\n",
    "        self.all_head_size = self.attn_head_size * num_heads\n",
    "\n",
    "        w_attr_1, b_attr_1 = self._init_weights()\n",
    "        self.qkv = nn.Linear(embed_dim,\n",
    "                             self.all_head_size * 3,  # weights for q, k, and v\n",
    "                             weight_attr=w_attr_1,\n",
    "                             bias_attr=b_attr_1 if qkv_bias else False)\n",
    "\n",
    "        self.scales = self.attn_head_size ** -0.5\n",
    "\n",
    "        w_attr_2, b_attr_2 = self._init_weights()\n",
    "        self.out = nn.Linear(self.all_head_size,\n",
    "                             embed_dim,\n",
    "                             weight_attr=w_attr_2,\n",
    "                             bias_attr=b_attr_2)\n",
    "\n",
    "        self.attn_dropout = nn.Dropout(attention_dropout)\n",
    "        self.proj_dropout = nn.Dropout(dropout)\n",
    "        self.softmax = nn.Softmax(axis=-1)\n",
    "        self.attn_weights = None\n",
    "\n",
    "    def _init_weights(self):\n",
    "        weight_attr = paddle.ParamAttr(initializer=nn.initializer.TruncatedNormal(std=.02))\n",
    "        bias_attr = paddle.ParamAttr(initializer=nn.initializer.Constant(0.0))\n",
    "        return weight_attr, bias_attr\n",
    "\n",
    "    def transpose_multihead(self, x):\n",
    "        \"\"\"[B, N, C] -> [B, N, n_heads, head_dim] -> [B, n_heads, N, head_dim]\"\"\"\n",
    "        new_shape = x.shape[:-1] + [self.num_heads, self.attn_head_size]\n",
    "        x = x.reshape(new_shape)  # [B, N, C] -> [B, N, n_heads, head_dim]\n",
    "        x = x.transpose([0, 2, 1, 3])  # [B, N, n_heads, head_dim] -> [B, n_heads, N, head_dim]\n",
    "        return x\n",
    "\n",
    "    def forward(self, x):\n",
    "        qkv = self.qkv(x).chunk(3, axis=-1)\n",
    "        q, k, v = map(self.transpose_multihead, qkv)\n",
    "\n",
    "        q = q * self.scales\n",
    "        attn = paddle.matmul(q, k, transpose_y=True)  # [B, n_heads, N, N]\n",
    "        attn = self.softmax(attn)\n",
    "        attn = self.attn_dropout(attn)\n",
    "        self.attn_weights = attn.detach()\n",
    "\n",
    "        z = paddle.matmul(attn, v)  # [B, n_heads, N, head_dim]\n",
    "        z = z.transpose([0, 2, 1, 3])  # [B, N, n_heads, head_dim]\n",
    "        new_shape = z.shape[:-2] + [self.all_head_size]\n",
    "        z = z.reshape(new_shape)  # [B, N, all_head_size]\n",
    "\n",
    "        z = self.out(z)\n",
    "        z = self.proj_dropout(z)\n",
    "        return z\n",
    "\n",
    "\n",
    "class Mlp(nn.Layer):\n",
    "    \"\"\" MLP module\n",
    "    Impl using nn.Linear and activation is GELU, dropout is applied.\n",
    "    Ops: fc -> act -> dropout -> fc -> dropout\n",
    "\n",
    "    Attributes:\n",
    "        fc1: nn.Linear\n",
    "        fc2: nn.Linear\n",
    "        act: GELU\n",
    "        dropout: dropout after fc\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self,\n",
    "                 embed_dim,\n",
    "                 mlp_ratio,\n",
    "                 dropout=0.):\n",
    "        super().__init__()\n",
    "        w_attr_1, b_attr_1 = self._init_weights()\n",
    "        self.fc1 = nn.Linear(embed_dim,\n",
    "                             int(embed_dim * mlp_ratio),\n",
    "                             weight_attr=w_attr_1,\n",
    "                             bias_attr=b_attr_1)\n",
    "\n",
    "        w_attr_2, b_attr_2 = self._init_weights()\n",
    "        self.fc2 = nn.Linear(int(embed_dim * mlp_ratio),\n",
    "                             embed_dim,\n",
    "                             weight_attr=w_attr_2,\n",
    "                             bias_attr=b_attr_2)\n",
    "        self.act = nn.GELU()\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def _init_weights(self):\n",
    "        weight_attr = paddle.ParamAttr(\n",
    "            initializer=paddle.nn.initializer.TruncatedNormal(std=0.2))\n",
    "        bias_attr = paddle.ParamAttr(\n",
    "            initializer=paddle.nn.initializer.Constant(0.0))\n",
    "        return weight_attr, bias_attr\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.fc1(x)\n",
    "        x = self.act(x)\n",
    "        x = self.dropout(x)\n",
    "        x = self.fc2(x)\n",
    "        x = self.dropout(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "class TransformerLayer(nn.Layer):\n",
    "    \"\"\"Transformer Layer\n",
    "    Transformer layer contains attention, norm, mlp and residual\n",
    "\n",
    "    Attributes:\n",
    "        embed_dim: transformer feature dim\n",
    "        attn_norm: nn.LayerNorm before attention\n",
    "        mlp_norm: nn.LayerNorm before mlp\n",
    "        mlp: mlp modual\n",
    "        attn: attention modual\n",
    "    \"\"\"\n",
    "    def __init__(self,\n",
    "                 embed_dim,\n",
    "                 num_heads,\n",
    "                 attn_head_size=None,\n",
    "                 qkv_bias=True,\n",
    "                 mlp_ratio=4.,\n",
    "                 dropout=0.,\n",
    "                 attention_dropout=0.,\n",
    "                 droppath=0.):\n",
    "        super().__init__()\n",
    "        w_attr_1, b_attr_1 = self._init_weights()\n",
    "        self.attn_norm = nn.LayerNorm(embed_dim,\n",
    "                                      weight_attr=w_attr_1,\n",
    "                                      bias_attr=b_attr_1,\n",
    "                                      epsilon=1e-6)\n",
    "\n",
    "        self.attn = Attention(embed_dim,\n",
    "                              num_heads,\n",
    "                              attn_head_size,\n",
    "                              qkv_bias,\n",
    "                              dropout,\n",
    "                              attention_dropout)\n",
    "\n",
    "        #self.drop_path = DropPath(droppath) if droppath > 0. else Identity()\n",
    "\n",
    "        w_attr_2, b_attr_2 = self._init_weights()\n",
    "        self.mlp_norm = nn.LayerNorm(embed_dim,\n",
    "                                     weight_attr=w_attr_2,\n",
    "                                     bias_attr=b_attr_2,\n",
    "                                     epsilon=1e-6)\n",
    "\n",
    "        self.mlp = Mlp(embed_dim, mlp_ratio, dropout)\n",
    "\n",
    "    def _init_weights(self):\n",
    "        weight_attr = paddle.ParamAttr(initializer=nn.initializer.Constant(1.0))\n",
    "        bias_attr = paddle.ParamAttr(initializer=nn.initializer.Constant(0.0))\n",
    "        return weight_attr, bias_attr\n",
    "\n",
    "    def forward(self, x):\n",
    "        h = x\n",
    "        x = self.attn_norm(x)\n",
    "        x = self.attn(x)\n",
    "        #x = self.drop_path(x)\n",
    "        x = x + h\n",
    "\n",
    "        h = x\n",
    "        x = self.mlp_norm(x)\n",
    "        x = self.mlp(x)\n",
    "        #x = self.drop_path(x)\n",
    "        x = x + h\n",
    "\n",
    "        return x\n",
    "\n",
    "\n",
    "class Encoder(nn.Layer):\n",
    "    \"\"\"Transformer encoder\n",
    "    Encoder encoder contains a list of TransformerLayer, and a LayerNorm.\n",
    "\n",
    "    Attributes:\n",
    "        layers: nn.LayerList contains multiple EncoderLayers\n",
    "        encoder_norm: nn.LayerNorm which is applied after last encoder layer\n",
    "    \"\"\"\n",
    "    def __init__(self,\n",
    "                 embed_dim,\n",
    "                 num_heads,\n",
    "                 depth,\n",
    "                 attn_head_size=None,\n",
    "                 qkv_bias=True,\n",
    "                 mlp_ratio=4.0,\n",
    "                 dropout=0.,\n",
    "                 attention_dropout=0.,\n",
    "                 droppath=0.):\n",
    "        super().__init__()\n",
    "        # stochatic depth decay\n",
    "        depth_decay = [x.item() for x in paddle.linspace(0, droppath, depth)]\n",
    "\n",
    "        layer_list = []\n",
    "        for i in range(depth):\n",
    "            layer_list.append(TransformerLayer(embed_dim,\n",
    "                                               num_heads,\n",
    "                                               attn_head_size,\n",
    "                                               qkv_bias,\n",
    "                                               mlp_ratio,\n",
    "                                               dropout,\n",
    "                                               attention_dropout,\n",
    "                                               depth_decay[i]))\n",
    "        self.layers = nn.LayerList(layer_list)\n",
    "\n",
    "        w_attr_1, b_attr_1 = self._init_weights()\n",
    "        self.encoder_norm = nn.LayerNorm(embed_dim,\n",
    "                                         weight_attr=w_attr_1,\n",
    "                                         bias_attr=b_attr_1,\n",
    "                                         epsilon=1e-6)\n",
    "\n",
    "    def _init_weights(self):\n",
    "        weight_attr = paddle.ParamAttr(initializer=nn.initializer.Constant(1.0))\n",
    "        bias_attr = paddle.ParamAttr(initializer=nn.initializer.Constant(0.0))\n",
    "        return weight_attr, bias_attr\n",
    "\n",
    "    def forward(self, x):\n",
    "        for layer in self.layers:\n",
    "            x = layer(x)\n",
    "        x = self.encoder_norm(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "class VisionTransformer(nn.Layer):\n",
    "    \"\"\"ViT transformer\n",
    "    ViT Transformer, classifier is a single Linear layer for finetune,\n",
    "    For training from scratch, two layer mlp should be used.\n",
    "    Classification is done using cls_token.\n",
    "\n",
    "    Args:\n",
    "        image_size: int, input image size, default: 224\n",
    "        patch_size: int, patch size, default: 16\n",
    "        in_channels: int, input image channels, default: 3\n",
    "        num_classes: int, number of classes for classification, default: 1000\n",
    "        embed_dim: int, embedding dimension (patch embed out dim), default: 768\n",
    "        depth: int, number ot transformer blocks, default: 12\n",
    "        num_heads: int, number of attention heads, default: 12\n",
    "        attn_head_size: int, dim of head, if none, set to embed_dim // num_heads, default: None\n",
    "        mlp_ratio: float, ratio of mlp hidden dim to embed dim(mlp in dim), default: 4.0\n",
    "        qkv_bias: bool, If True, enable qkv(nn.Linear) layer with bias, default: True\n",
    "        dropout: float, dropout rate for linear layers, default: 0.\n",
    "        attention_dropout: float, dropout rate for attention layers default: 0.\n",
    "        droppath: float, droppath rate for droppath layers, default: 0.\n",
    "        representation_size: int, set representation layer (pre-logits) if set, default: None\n",
    "    \"\"\"\n",
    "    def __init__(self,\n",
    "                 image_size=224,\n",
    "                 patch_size=16,\n",
    "                 in_channels=3,\n",
    "                 num_classes=1000,\n",
    "                 embed_dim=768,\n",
    "                 depth=12,\n",
    "                 num_heads=12,\n",
    "                 attn_head_size=None,\n",
    "                 mlp_ratio=4,\n",
    "                 qkv_bias=True,\n",
    "                 dropout=0.,\n",
    "                 attention_dropout=0.,\n",
    "                 droppath=0.,\n",
    "                 representation_size=None):\n",
    "        super().__init__()\n",
    "        # create patch embedding\n",
    "        self.patch_embedding = PatchEmbedding(image_size,\n",
    "                                              patch_size,\n",
    "                                              in_channels,\n",
    "                                              embed_dim)\n",
    "        # create posision embedding\n",
    "        self.position_embedding = paddle.create_parameter(\n",
    "            shape=[1, 1 + self.patch_embedding.num_patches, embed_dim],\n",
    "            dtype='float32',\n",
    "            default_initializer=paddle.nn.initializer.TruncatedNormal(std=.02))\n",
    "        # create cls token\n",
    "        self.cls_token = paddle.create_parameter(\n",
    "            shape=[1, 1, embed_dim],\n",
    "            dtype='float32',\n",
    "            default_initializer=paddle.nn.initializer.TruncatedNormal(std=.02))\n",
    "        self.pos_dropout = nn.Dropout(dropout)\n",
    "        # create multi head self-attention layers\n",
    "        self.encoder = Encoder(embed_dim,\n",
    "                               num_heads,\n",
    "                               depth,\n",
    "                               attn_head_size,\n",
    "                               qkv_bias,\n",
    "                               mlp_ratio,\n",
    "                               dropout,\n",
    "                               attention_dropout,\n",
    "                               droppath)\n",
    "        # pre-logits\n",
    "        if representation_size is not None:\n",
    "            self.num_features = representation_size\n",
    "            w_attr_1, b_attr_1 = self._init_weights()\n",
    "            self.pre_logits = nn.Sequential(\n",
    "                nn.Linear(embed_dim,\n",
    "                          representation_size,\n",
    "                          weight_attr=w_attr_1,\n",
    "                          bias_attr=b_attr_1),\n",
    "                nn.ReLU())\n",
    "        else:\n",
    "            self.pre_logits = Identity()\n",
    "\n",
    "        # classifier head\n",
    "        w_attr_2, b_attr_2 = self._init_weights()\n",
    "        self.classifier = nn.Linear(embed_dim,\n",
    "                                    num_classes,\n",
    "                                    weight_attr=w_attr_2,\n",
    "                                    bias_attr=b_attr_2)\n",
    "\n",
    "    def _init_weights(self):\n",
    "        weight_attr = paddle.ParamAttr(\n",
    "            initializer=paddle.nn.initializer.Constant(1.0))\n",
    "        bias_attr = paddle.ParamAttr(\n",
    "            initializer=paddle.nn.initializer.Constant(0.0))\n",
    "        return weight_attr, bias_attr\n",
    "\n",
    "    def forward_features(self, x):\n",
    "        x = self.patch_embedding(x)\n",
    "        cls_tokens = self.cls_token.expand((x.shape[0], -1, -1))\n",
    "        x = paddle.concat((cls_tokens, x), axis=1)\n",
    "        x = x + self.position_embedding\n",
    "        x = self.pos_dropout(x)\n",
    "        x = self.encoder(x)\n",
    "        x = self.pre_logits(x[:, 0]) # cls_token only\n",
    "        return x\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.forward_features(x)\n",
    "        logits = self.classifier(x)\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "06feb06f-dbab-427f-b224-c212d925224b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-08-19T13:29:05.762243Z",
     "iopub.status.busy": "2024-08-19T13:29:05.761463Z",
     "iopub.status.idle": "2024-08-19T13:29:05.775587Z",
     "shell.execute_reply": "2024-08-19T13:29:05.774701Z",
     "shell.execute_reply.started": "2024-08-19T13:29:05.762184Z"
    },
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "class MetricsCalculator(object):  \n",
    "    def __init__(self):  \n",
    "        self.TP = 0  \n",
    "        self.FP = 0  \n",
    "        self.FN = 0  \n",
    "        self.TN = 0\n",
    "        self.y_trues = []\n",
    "        self.y_pred_onehot = []\n",
    "        self.y_preds_proba = []\n",
    "  \n",
    "    def update(self, y_true, y_pred, y_pred_onehot, y_pred_proba):  \n",
    "        y_true = np.array(y_true, dtype='float64').reshape(-1, 1)  \n",
    "        y_pred = np.array(y_pred, dtype='float64').reshape(-1, 1)\n",
    "        y_pred_onehot = np.array(y_pred_onehot, dtype='float64').reshape(-1, 2)\n",
    "        y_pred_proba = np.array(y_pred_proba, dtype='float64').reshape(-1, 2)\n",
    "\n",
    "        self.y_trues.extend(y_true)\n",
    "        self.y_pred_onehot.extend(y_pred_onehot)\n",
    "        self.y_preds_proba.extend(y_pred_proba)  # Update the correct variable here\n",
    "  \n",
    "        # 假设 y_true 中 1 表示正类，0 表示负类  \n",
    "        self.TP += np.sum((y_true == 1) & (y_pred == 1)) \n",
    "        self.FN += np.sum((y_true == 1) & (y_pred == 0))\n",
    "        self.FP += np.sum((y_true == 0) & (y_pred == 1))\n",
    "        self.TN += np.sum((y_true == 0) & (y_pred == 0))\n",
    "\n",
    "    def calculate_brier_score(self):\n",
    "        y_true = np.array(self.y_pred_onehot)\n",
    "        y_pred_proba = np.array(self.y_preds_proba)  # Use the correct variable here\n",
    "\n",
    "        # 计算 BS\n",
    "        BS = np.mean((y_true - y_pred_proba) ** 2, axis=0)\n",
    "        BS = BS[1]\n",
    "\n",
    "        # 计算 BSS\n",
    "        y_mean = np.mean(y_true, axis=0)\n",
    "        reference_bs = np.mean((y_true - y_mean) ** 2, axis=0)\n",
    "        reference_bs = np.mean((y_true[:, 1] - y_mean[1]) ** 2, axis=0)\n",
    "        BSS = 1 - BS / reference_bs if reference_bs != 0 else 0\n",
    "\n",
    "        return BS, BSS\n",
    "\n",
    "    def calculate_metrics(self):  \n",
    "        total = self.TP + self.FN + self.FP + self.TN  \n",
    "  \n",
    "        Accuracy = (self.TP + self.TN) / total if total > 0 else 0  \n",
    "        Precision = self.TP / (self.TP + self.FP) if (self.TP + self.FP) > 0 else 0  \n",
    "        Recall = self.TP / (self.TP + self.FN) if (self.TP + self.FN) > 0 else 0  \n",
    "        FAR = self.FP / (self.FP + self.TP) if (self.FP + self.TP) > 0 else 0  \n",
    "        TSS = Recall - (self.FP / (self.FP + self.TN)) if (self.FP + self.TN) > 0 else 0\n",
    "        HSS = (2 * (self.TP * self.TN - self.FP * self.FN)) / ((self.TP + self.FN) * (self.FN + self.TN) + (self.TP + self.FP) * (self.FP + self.TN) + 1e-5)  \n",
    "        \n",
    "        BS, BSS = self.calculate_brier_score()\n",
    "  \n",
    "        metrics = {  \n",
    "            'TP': self.TP,  \n",
    "            'FP': self.FP,  \n",
    "            'FN': self.FN,  \n",
    "            'TN': self.TN,  \n",
    "            'Accuracy': Accuracy,  \n",
    "            'Precision': Precision,  \n",
    "            'Recall': Recall,  \n",
    "            'FAR': FAR,  \n",
    "            'TSS': TSS,  \n",
    "            'HSS': HSS,\n",
    "            'Brier Score (BS)': BS,\n",
    "            'Brier Skill Score (BSS)': BSS\n",
    "        }  \n",
    "        return metrics "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fd713240-1bce-47d3-99c4-ba6777b522e6",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-08-19T13:22:38.406739Z",
     "iopub.status.busy": "2024-08-19T13:22:38.405881Z",
     "iopub.status.idle": "2024-08-19T13:22:38.414540Z",
     "shell.execute_reply": "2024-08-19T13:22:38.413664Z",
     "shell.execute_reply.started": "2024-08-19T13:22:38.406696Z"
    },
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "def data_preprocess(data_path, chunk_size=1000):\n",
    "    labels_list = []\n",
    "    images_list = []\n",
    "\n",
    "    # Process data in chunks to save memory\n",
    "    for chunk in pd.read_csv(data_path, header=None, usecols=[3] + list(range(4, 16388)), chunksize=chunk_size):\n",
    "        labels_chunk = chunk.iloc[:, 0].values\n",
    "        images_chunk = chunk.iloc[:, 1:].values.astype('float32').reshape(-1, 128, 128)\n",
    "        \n",
    "        # Convert labels to binary\n",
    "        label_mapping = {'N': 0, 'C': 0, 'M': 1, 'X': 1}\n",
    "        labels_chunk = np.vectorize(label_mapping.get)(labels_chunk).astype('float32').reshape(-1, 1)\n",
    "        \n",
    "        # Resize and preprocess images\n",
    "        images_resized = np.array([cv2.resize(img, (224, 224), interpolation=cv2.INTER_LINEAR) for img in images_chunk])\n",
    "        images_resized = np.stack([images_resized]*3, axis=-1)  # Convert to 3 channels\n",
    "        images_resized = images_resized.transpose([0, 3, 1, 2])  # Change to (num_samples, channels, height, width)\n",
    "        images_resized /= 4000.0  # Normalize\n",
    "\n",
    "        labels_list.append(labels_chunk)\n",
    "        images_list.append(images_resized)\n",
    "\n",
    "        # Clear memory\n",
    "        del chunk, labels_chunk, images_chunk, images_resized\n",
    "        gc.collect()\n",
    "\n",
    "    labels = np.vstack(labels_list)\n",
    "    images = np.vstack(images_list)\n",
    "\n",
    "    # Clear memory\n",
    "    del labels_list, images_list\n",
    "    gc.collect()\n",
    "\n",
    "    return images, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7d06d597-240d-44d4-9d7b-74ef2b664906",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-08-19T13:22:40.573919Z",
     "iopub.status.busy": "2024-08-19T13:22:40.573180Z",
     "iopub.status.idle": "2024-08-19T13:22:42.706316Z",
     "shell.execute_reply": "2024-08-19T13:22:42.705477Z",
     "shell.execute_reply.started": "2024-08-19T13:22:40.573876Z"
    },
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0819 21:22:40.576051 31808 gpu_resources.cc:119] Please NOTE: device: 0, GPU Compute Capability: 7.0, Driver API Version: 12.0, Runtime API Version: 11.8\r\n",
      "W0819 21:22:40.577553 31808 gpu_resources.cc:164] device: 0, cuDNN Version: 8.9.\r\n"
     ]
    }
   ],
   "source": [
    "paddle.seed(40)\n",
    "np.random.seed(42)\n",
    "paddle.device.cuda.empty_cache()\n",
    "weight = paddle.to_tensor([0.569, 4.120], dtype='float32')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcf7e245-8a35-44ec-a1f7-f44739eac51a",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "batch_val = 20\n",
    "Train_Loss = []\n",
    "Val_Loss = []\n",
    "\n",
    "for dataset_id in range(10):\n",
    "    best_TSS = -1\n",
    "    # Data preprocessing\n",
    "    train_data, train_label = data_preprocess(f'dataset/DATA/feature/group9_Data2_image/{dataset_id}Train.csv')\n",
    "    val_data, val_label = data_preprocess(f'dataset/DATA/feature/group9_Data2_image/Sift/{dataset_id}Val.csv')\n",
    "\n",
    "    paddle.device.cuda.empty_cache()\n",
    "    gc.collect()\n",
    "\n",
    "    train_dataset = TensorDataset([train_data, train_label])\n",
    "    val_dataset = TensorDataset([val_data, val_label])\n",
    "    train_loader = DataLoader(train_dataset, batch_size=712, shuffle=True, drop_last=True)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=128, shuffle=True)\n",
    "\n",
    "    model = VisionTransformer(image_size=224,\n",
    "                              patch_size=16,\n",
    "                              in_channels=3,\n",
    "                              num_classes=1000,\n",
    "                              embed_dim=192,\n",
    "                              depth=12,\n",
    "                              num_heads=3,\n",
    "                              attn_head_size=None,\n",
    "                              mlp_ratio=4.0,\n",
    "                              qkv_bias=True,\n",
    "                              dropout=0.01,\n",
    "                              attention_dropout=0.01,\n",
    "                              droppath=0.,\n",
    "                              representation_size=None)\n",
    "    param = paddle.load('vit_tiny_patch16_224.pdparams')\n",
    "    model.load_dict(param)\n",
    "    model.classifier = paddle.nn.Linear(192, 2)  \n",
    "\n",
    "    scheduler = paddle.optimizer.lr.LinearWarmup(\n",
    "        learning_rate=1e-5, \n",
    "        warmup_steps=20, \n",
    "        start_lr=1e-7, \n",
    "        end_lr=1e-5, \n",
    "        verbose=False)\n",
    "    opt = paddle.optimizer.AdamW(\n",
    "        learning_rate=scheduler, \n",
    "        weight_decay=0.15, \n",
    "        parameters=model.parameters())\n",
    "\n",
    "    for i in range(30):\n",
    "        batch_id = 0\n",
    "        total_loss = 0\n",
    "        model.train()\n",
    "\n",
    "        for img, label in train_loader:\n",
    "            batch_id += 1\n",
    "            label = label.astype('int32')\n",
    "\n",
    "            pred = model(img)\n",
    "            loss = F.cross_entropy(pred, label, weight=weight)\n",
    "            total_loss += loss.item()\n",
    "\n",
    "            opt.clear_gradients()\n",
    "            loss.backward()\n",
    "            opt.step()\n",
    "\n",
    "            if batch_id % batch_val == 0:\n",
    "                print(f'Epoch: {i+1}')\n",
    "                print(f'Train_Loss: {total_loss / batch_val}')\n",
    "                Train_Loss.append(total_loss / batch_val)\n",
    "                total_loss = 0\n",
    "\n",
    "                TSS = []\n",
    "                val_batch_id = 0\n",
    "                calculator = MetricsCalculator()\n",
    "                model.eval()\n",
    "\n",
    "                with paddle.no_grad():\n",
    "                    for img, label in val_loader:\n",
    "                        val_batch_id += 1\n",
    "                        pred = model(img)\n",
    "                        label = label.astype('int32')\n",
    "                        loss = F.cross_entropy(pred, label, weight=weight)\n",
    "                        total_loss += loss.item()\n",
    "                        pred_label = paddle.argmax(pred, axis=-1)\n",
    "                        calculator.update(y_true=label, y_pred=pred_label)\n",
    "\n",
    "                val_loss_avg = total_loss / val_batch_id\n",
    "                print(f'Val_Loss: {val_loss_avg}')\n",
    "                Val_Loss.append(val_loss_avg)\n",
    "                total_loss = 0\n",
    "\n",
    "                metric = calculator.calculate_metrics()\n",
    "                if best_TSS < metric['TSS']:\n",
    "                    best_TSS = metric['TSS']\n",
    "                    paddle.save(model.state_dict(), f'model/Transformer_sift/ViT_{dataset_id}.pdparam')\n",
    "                print(metric)\n",
    "                print('Mean TSS: {}   Best_TSS:{}\\n'.format(metric['TSS'], best_TSS))\n",
    "\n",
    "                # Clear CUDA cache and collect garbage\n",
    "                paddle.device.cuda.empty_cache()\n",
    "                gc.collect()\n",
    "\n",
    "        scheduler.step()\n",
    "\n",
    "    loss_folder = 'Loss/ViT_sift/'\n",
    "    if not os.path.exists(loss_folder):\n",
    "        os.makedirs(loss_folder)\n",
    "    np.save(loss_folder + f'Train_Loss_{dataset_id}.npy', np.array(Train_Loss))\n",
    "    np.save(loss_folder + f'Val_Loss_{dataset_id}.npy', np.array(Val_Loss))    \n",
    "\n",
    "    # Clear CUDA cache and collect garbage after each dataset\n",
    "    paddle.device.cuda.empty_cache()\n",
    "    gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adbf8bd5-a6e7-41cd-b957-1e5060a5638e",
   "metadata": {},
   "source": [
    "## Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "aef92474-6cb2-41a1-9bbc-9c3a8d73bbe6",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-08-19T13:29:11.776898Z",
     "iopub.status.busy": "2024-08-19T13:29:11.776190Z",
     "iopub.status.idle": "2024-08-19T13:38:41.958508Z",
     "shell.execute_reply": "2024-08-19T13:38:41.957382Z",
     "shell.execute_reply.started": "2024-08-19T13:29:11.776858Z"
    },
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'TP': 27, 'FP': 21, 'FN': 8, 'TN': 119, 'Accuracy': 0.8342857142857143, 'Precision': 0.5625, 'Recall': 0.7714285714285715, 'FAR': 0.4375, 'TSS': 0.6214285714285714, 'HSS': 0.5454545449660058, 'Brier Score (BS)': 0.13057568534762004, 'Brier Skill Score (BSS)': 0.1839019665773749}\r\n",
      "{'TP': 30, 'FP': 22, 'FN': 5, 'TN': 118, 'Accuracy': 0.8457142857142858, 'Precision': 0.5769230769230769, 'Recall': 0.8571428571428571, 'FAR': 0.4230769230769231, 'TSS': 0.7, 'HSS': 0.5921450145946094, 'Brier Score (BS)': 0.1132763539787552, 'Brier Skill Score (BSS)': 0.2920227876327801}\r\n",
      "{'TP': 29, 'FP': 24, 'FN': 6, 'TN': 116, 'Accuracy': 0.8285714285714286, 'Precision': 0.5471698113207547, 'Recall': 0.8285714285714286, 'FAR': 0.4528301886792453, 'TSS': 0.6571428571428573, 'HSS': 0.5508982031215585, 'Brier Score (BS)': 0.14317451286201258, 'Brier Skill Score (BSS)': 0.10515929461242157}\r\n",
      "{'TP': 29, 'FP': 33, 'FN': 6, 'TN': 107, 'Accuracy': 0.7771428571428571, 'Precision': 0.46774193548387094, 'Recall': 0.8285714285714286, 'FAR': 0.532258064516129, 'TSS': 0.592857142857143, 'HSS': 0.4598337946499139, 'Brier Score (BS)': 0.17389281484132818, 'Brier Skill Score (BSS)': -0.086830092758301}\r\n",
      "{'TP': 25, 'FP': 21, 'FN': 10, 'TN': 119, 'Accuracy': 0.8228571428571428, 'Precision': 0.5434782608695652, 'Recall': 0.7142857142857143, 'FAR': 0.45652173913043476, 'TSS': 0.5642857142857143, 'HSS': 0.5047923318075834, 'Brier Score (BS)': 0.12805901899525496, 'Brier Skill Score (BSS)': 0.1996311312796567}\r\n",
      "{'TP': 26, 'FP': 17, 'FN': 9, 'TN': 123, 'Accuracy': 0.8514285714285714, 'Precision': 0.6046511627906976, 'Recall': 0.7428571428571429, 'FAR': 0.3953488372093023, 'TSS': 0.6214285714285714, 'HSS': 0.5723684205146914, 'Brier Score (BS)': 0.1342803475454643, 'Brier Skill Score (BSS)': 0.1607478278408483}\r\n",
      "{'TP': 21, 'FP': 13, 'FN': 14, 'TN': 127, 'Accuracy': 0.8457142857142858, 'Precision': 0.6176470588235294, 'Recall': 0.6, 'FAR': 0.38235294117647056, 'TSS': 0.5071428571428571, 'HSS': 0.5126353785326092, 'Brier Score (BS)': 0.1046433896526245, 'Brier Skill Score (BSS)': 0.34597881467109703}\r\n",
      "{'TP': 27, 'FP': 19, 'FN': 8, 'TN': 121, 'Accuracy': 0.8457142857142858, 'Precision': 0.5869565217391305, 'Recall': 0.7714285714285715, 'FAR': 0.41304347826086957, 'TSS': 0.6357142857142858, 'HSS': 0.5686900953275308, 'Brier Score (BS)': 0.11205479642143118, 'Brier Skill Score (BSS)': 0.29965752236605525}\r\n",
      "{'TP': 29, 'FP': 20, 'FN': 6, 'TN': 120, 'Accuracy': 0.8514285714285714, 'Precision': 0.5918367346938775, 'Recall': 0.8285714285714286, 'FAR': 0.40816326530612246, 'TSS': 0.6857142857142857, 'HSS': 0.5962732913963857, 'Brier Score (BS)': 0.12296593295444176, 'Brier Skill Score (BSS)': 0.23146291903473915}\r\n",
      "{'TP': 31, 'FP': 29, 'FN': 4, 'TN': 111, 'Accuracy': 0.8114285714285714, 'Precision': 0.5166666666666667, 'Recall': 0.8857142857142857, 'FAR': 0.48333333333333334, 'TSS': 0.6785714285714285, 'HSS': 0.5352112671748803, 'Brier Score (BS)': 0.1396483835675229, 'Brier Skill Score (BSS)': 0.12719760270298197}\r\n"
     ]
    }
   ],
   "source": [
    "for dataset_id in range(10):\n",
    "    test_data, test_label = data_preprocess(f'dataset/DATA/feature/group9_Data2_image/{dataset_id}Test.csv')\n",
    "    test_data = test_data[39::40, ...]\n",
    "    test_label = test_label[39::40, ...]\n",
    "    test_dataset = TensorDataset([test_data, test_label])\n",
    "    test_loader = DataLoader(test_dataset, batch_size=100, shuffle=False)\n",
    "    model = VisionTransformer(image_size=224,\n",
    "                              patch_size=16,\n",
    "                              in_channels=3,\n",
    "                              num_classes=2,\n",
    "                              embed_dim=192,\n",
    "                              depth=12,\n",
    "                              num_heads=3,\n",
    "                              attn_head_size=None,\n",
    "                              mlp_ratio=4.0,\n",
    "                              qkv_bias=True,\n",
    "                              dropout=0.01,\n",
    "                              attention_dropout=0.01,\n",
    "                              droppath=0.,\n",
    "                              representation_size=None)\n",
    "    calculator = MetricsCalculator()\n",
    "    param = paddle.load(f'model/Transformer/ViT_{dataset_id}.pdparam')\n",
    "    model.load_dict(param)\n",
    "    model.eval()\n",
    "\n",
    "    TSS = []\n",
    "    with paddle.no_grad():\n",
    "        for img, label in test_loader:\n",
    "            pred = model(img)\n",
    "            label = label.astype('long')\n",
    "            pred_label = paddle.argmax(pred, axis=-1)\n",
    "            pred_onehot = F.one_hot(label, 2)\n",
    "            pred_proba = F.softmax(pred, axis=-1)\n",
    "            calculator.update(label, pred_label.detach().cpu().numpy(), pred_onehot.detach().cpu().numpy(), pred_proba.detach().cpu().numpy())\n",
    "    metric = calculator.calculate_metrics()\n",
    "    print(metric)\n",
    "\n",
    "    metric_folder = 'Metrics_timestep_40/ViT_BSS/'\n",
    "    if not os.path.exists(metric_folder):\n",
    "        os.makedirs(metric_folder)\n",
    "    data_serializable = {k: int(v) if isinstance(v, np.integer) else v for k, v in metric.items()} \n",
    "    with open(metric_folder + f'dataset_{dataset_id}.json', 'w', encoding='utf-8') as f:  \n",
    "        json.dump(data_serializable, f, ensure_ascii=True, indent=4)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "bf116a60-488a-4d87-953b-25ce3aafbba9",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-10T14:54:42.578083Z",
     "iopub.status.busy": "2024-10-10T14:54:42.577482Z",
     "iopub.status.idle": "2024-10-10T14:54:42.591315Z",
     "shell.execute_reply": "2024-10-10T14:54:42.590368Z",
     "shell.execute_reply.started": "2024-10-10T14:54:42.578033Z"
    },
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'TP': 27, 'FP': 21, 'FN': 8, 'TN': 119, 'Accuracy': 0.8342857142857143, 'Precision': 0.5625, 'Recall': 0.7714285714285715, 'FAR': 0.4375, 'TSS': 0.6214285714285714, 'HSS': 0.5454545449660058, 'Brier Score (BS)': 0.13057568534762004, 'Brier Skill Score (BSS)': 0.1839019665773749}\r\n",
      "{'TP': 30, 'FP': 22, 'FN': 5, 'TN': 118, 'Accuracy': 0.8457142857142858, 'Precision': 0.5769230769230769, 'Recall': 0.8571428571428571, 'FAR': 0.4230769230769231, 'TSS': 0.7, 'HSS': 0.5921450145946094, 'Brier Score (BS)': 0.1132763539787552, 'Brier Skill Score (BSS)': 0.2920227876327801}\r\n",
      "{'TP': 29, 'FP': 24, 'FN': 6, 'TN': 116, 'Accuracy': 0.8285714285714286, 'Precision': 0.5471698113207547, 'Recall': 0.8285714285714286, 'FAR': 0.4528301886792453, 'TSS': 0.6571428571428573, 'HSS': 0.5508982031215585, 'Brier Score (BS)': 0.14317451286201258, 'Brier Skill Score (BSS)': 0.10515929461242157}\r\n",
      "{'TP': 29, 'FP': 33, 'FN': 6, 'TN': 107, 'Accuracy': 0.7771428571428571, 'Precision': 0.46774193548387094, 'Recall': 0.8285714285714286, 'FAR': 0.532258064516129, 'TSS': 0.592857142857143, 'HSS': 0.4598337946499139, 'Brier Score (BS)': 0.17389281484132818, 'Brier Skill Score (BSS)': -0.086830092758301}\r\n",
      "{'TP': 25, 'FP': 21, 'FN': 10, 'TN': 119, 'Accuracy': 0.8228571428571428, 'Precision': 0.5434782608695652, 'Recall': 0.7142857142857143, 'FAR': 0.45652173913043476, 'TSS': 0.5642857142857143, 'HSS': 0.5047923318075834, 'Brier Score (BS)': 0.12805901899525496, 'Brier Skill Score (BSS)': 0.1996311312796567}\r\n",
      "{'TP': 26, 'FP': 17, 'FN': 9, 'TN': 123, 'Accuracy': 0.8514285714285714, 'Precision': 0.6046511627906976, 'Recall': 0.7428571428571429, 'FAR': 0.3953488372093023, 'TSS': 0.6214285714285714, 'HSS': 0.5723684205146914, 'Brier Score (BS)': 0.1342803475454643, 'Brier Skill Score (BSS)': 0.1607478278408483}\r\n",
      "{'TP': 21, 'FP': 13, 'FN': 14, 'TN': 127, 'Accuracy': 0.8457142857142858, 'Precision': 0.6176470588235294, 'Recall': 0.6, 'FAR': 0.38235294117647056, 'TSS': 0.5071428571428571, 'HSS': 0.5126353785326092, 'Brier Score (BS)': 0.1046433896526245, 'Brier Skill Score (BSS)': 0.34597881467109703}\r\n",
      "{'TP': 27, 'FP': 19, 'FN': 8, 'TN': 121, 'Accuracy': 0.8457142857142858, 'Precision': 0.5869565217391305, 'Recall': 0.7714285714285715, 'FAR': 0.41304347826086957, 'TSS': 0.6357142857142858, 'HSS': 0.5686900953275308, 'Brier Score (BS)': 0.11205479642143118, 'Brier Skill Score (BSS)': 0.29965752236605525}\r\n",
      "{'TP': 29, 'FP': 20, 'FN': 6, 'TN': 120, 'Accuracy': 0.8514285714285714, 'Precision': 0.5918367346938775, 'Recall': 0.8285714285714286, 'FAR': 0.40816326530612246, 'TSS': 0.6857142857142857, 'HSS': 0.5962732913963857, 'Brier Score (BS)': 0.12296593295444176, 'Brier Skill Score (BSS)': 0.23146291903473915}\r\n",
      "{'TP': 31, 'FP': 29, 'FN': 4, 'TN': 111, 'Accuracy': 0.8114285714285714, 'Precision': 0.5166666666666667, 'Recall': 0.8857142857142857, 'FAR': 0.48333333333333334, 'TSS': 0.6785714285714285, 'HSS': 0.5352112671748803, 'Brier Score (BS)': 0.1396483835675229, 'Brier Skill Score (BSS)': 0.12719760270298197}\r\n",
      "10\r\n",
      "Accuracy: 0.8314285714285713 0.02202039870628309\r\n",
      "Percision: 0.5615571229311169 0.04260568686877711\r\n",
      "Recall: 0.7828571428571427 0.07897299977763002\r\n",
      "FAR: 0.4384428770688831 0.0426056868687771\r\n",
      "FPR: 0.15642857142857142 0.038789068521101325\r\n",
      "TSS: 0.6264285714285716 0.0565189150205667\r\n",
      "HSS: 0.5438302342085768 0.04024474237613667\r\n",
      "BSS: 0.1858929773959654 0.11707943041929633\r\n",
      "BS : 0.13025712361664554 0.018732708867087416\r\n"
     ]
    }
   ],
   "source": [
    "read_folder = 'Metrics_timestep_40/ViT_BSS/'\n",
    "read_file = sorted(os.listdir(read_folder))[:]\n",
    "TSS = []\n",
    "Accuracy = []\n",
    "Recall = []\n",
    "FAR = []\n",
    "Percision = []\n",
    "HSS = []\n",
    "BSS = []\n",
    "BS = []\n",
    "FPR = []\n",
    "for r_file in read_file:\n",
    "    r_file = os.path.join(read_folder, r_file)\n",
    "    with open(r_file, 'r') as f:\n",
    "        data = json.load(f)\n",
    "        print(data)\n",
    "        FP = data['FP']\n",
    "        TN = data['TN']\n",
    "        FPR.append(FP / (FP + TN))\n",
    "        TSS.append(data['TSS'])\n",
    "        Accuracy.append(data['Accuracy'])\n",
    "        Percision.append(data['Precision'])\n",
    "        Recall.append(data['Recall'])\n",
    "        FAR.append(data['FAR'])\n",
    "        HSS.append(data['HSS'])\n",
    "        BSS.append(data['Brier Skill Score (BSS)'])\n",
    "        BS.append(data['Brier Score (BS)'])\n",
    "print(len(Recall))\n",
    "print('Accuracy:', np.mean(Accuracy), np.std(Accuracy))\n",
    "print('Percision:', np.mean(Percision), np.std(Percision))\n",
    "print('Recall:', np.mean(Recall), np.std(Recall))\n",
    "print('FAR:', np.mean(FAR), np.std(FAR))\n",
    "print('FPR:', np.mean(FPR), np.std(FPR))\n",
    "print('TSS:', np.mean(TSS), np.std(TSS))\n",
    "print('HSS:', np.mean(HSS), np.std(HSS))\n",
    "print('BSS:', np.mean(BSS), np.std(BSS))\n",
    "print('BS :', np.mean(BS), np.std(BS))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "py35-paddle1.2.0"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
